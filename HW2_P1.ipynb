{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 2. #Профилирование пользователей. Сегментация аудитории: unsupervised learning (clustering, LDA/ARTM), supervised (multi/binary classification)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "План занятия:\n",
    "\n",
    "1. задача сегментации аудитории по интересам - для чего\n",
    "2. тематическое моделирование - получаем эмбединги текстов\n",
    "3. решаем downstream-задачу (профилирование аудитории новостного портала)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Переходим к практике"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим пользователей и списки последних прочитанных новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нам нужно получить векторные представления пользователей на основе прочитанным ими новостей и самих новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Получаем векторные представления новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/helgi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#предобработка текстов\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "#from nltk.tokenize import word_tokenize\n",
    "\n",
    "from razdel import tokenize # https://github.com/natasha/razdel\n",
    "#!pip install razdel\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sw = nltk.download('stopwords')\n",
    "stopword_ru = stopwords.words('russian')\n",
    "#stopword_ru = sw.words('russian')\n",
    "len(stopword_ru)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.7 s, sys: 907 ms, total: 21.6 s\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 34s, sys: 150 ms, total: 2min 34s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь в 3 строчки обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое common_dictionary и как он выглядит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ватутин'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все просто - это словарь наших слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 21s, sys: 3min 35s, total: 4min 56s\n",
      "Wall time: 42.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model1.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.1457878),\n",
       " (8, 0.18848033),\n",
       " (11, 0.17453343),\n",
       " (19, 0.3147755),\n",
       " (23, 0.16633725)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda[common_corpus[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущее значение: [(1, 0.20295064), (6, 0.61372125), (16, 0.07567507), (21, 0.09709107)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучили модель. Теперь 2 вопроса:\n",
    "\n",
    "1. как выглядят наши темы\n",
    "2. как получить для документа вектор значений (вероятности принадлежности каждой теме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['матч', 'финал', 'кубок', 'россия', 'футбол', 'приостановить', 'судья', 'изз', 'взрыв', 'пиротехнический', 'снаряд', 'передавать', 'корреспондент', 'газета', 'ru', 'болельщик', 'выбросить', 'поле', 'петарда', 'судья', 'увести', 'команда', 'поле', 'подтрибунный', 'помещение', 'динамовец', 'уйти', 'торпедовец', 'остаться', 'кромка', 'поле', 'матч', 'остановить', 'пять', 'минута', 'газета', 'ru', 'вести', 'онлайнтрансляция', 'матч']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.045061342),\n",
       " (7, 0.03488643),\n",
       " (11, 0.10959133),\n",
       " (12, 0.10646352),\n",
       " (17, 0.1494015),\n",
       " (19, 0.09820697),\n",
       " (21, 0.442208)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[0]\n",
    "print(other_texts[1])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[(0, 0.01710771),\n",
    " (5, 0.2652748),\n",
    " (6, 0.5154706),\n",
    " (10, 0.06833402),\n",
    " (15, 0.106660455),\n",
    " (21, 0.019863937)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.14479233),\n",
       " (8, 0.18904553),\n",
       " (11, 0.17446014),\n",
       " (19, 0.31515992),\n",
       " (23, 0.16645643)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ot = [t for t in news['title'][:4]]\n",
    "oc = [common_dictionary.doc2bow(text) for text in ot]\n",
    "lda[oc[3]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: лауреат звание лесной компенсировать достопримечательность норматив свести\n",
      "topic_1: россия nn исследование всё новый проект случай\n",
      "topic_2: товар торговый аэропорт участок девочка порт терминал\n",
      "topic_3: земля мозг наука обнаружить комиссия способ мышь\n",
      "topic_4: смерть возраст умереть скончаться жизнь родственник подросток\n",
      "topic_5: военный сша операция американский статья первый россия\n",
      "topic_6: снижение смерть кость организм подниматься звёздный переносить\n",
      "topic_7: тело источник египет данные участок якобы восток\n",
      "topic_8: университет россия всё новый российский фестиваль система\n",
      "topic_9: пенсия препарат след ск обнаружить следователь применение\n",
      "topic_10: ребёнок nn население эксперт исследование российский помощь\n",
      "topic_11: nn планета солнце россиянин достигать день виза\n",
      "topic_12: украина украинский санкция россия российский nn киев\n",
      "topic_13: иран необычный великобритания дания адмирал медленно ирландия\n",
      "topic_14: экономика экономический гражданин рост бизнес значительно знаменитый\n",
      "topic_15: медицина выручка приобретать интересовать вслед вечеринка пропуск\n",
      "topic_16: космос налог доллар млн зуб платёжный ричард\n",
      "topic_17: россия глава путин российский министерство рф управление\n",
      "topic_18: банк вирус реформа энергия механизм граница транспорт\n",
      "topic_19: рубль млн закон тыс орган размер сотрудник\n",
      "topic_20: американский сша писать гражданин снижение дом болезнь\n",
      "topic_21: млрд цена млн журнал научный ракета новый\n",
      "topic_22: фонд конкурс супруг мальчик дикий стресс победитель\n",
      "topic_23: газ хороший производить оплата энергия употребление летний\n",
      "topic_24: место произойти nn погибнуть экипаж первый обнаружить\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topic_0: сотрудник убийство орган следователь риа подозревать правоохранительный\n",
    "topic_1: солнце атмосферный малое нервный спасти кольцо экспериментальный\n",
    "topic_2: фонд обнаружить nn автор смерть найти препарат\n",
    "topic_3: рубль тыс выплата млн налоговый сумма уголовный\n",
    "topic_4: исследование научный университет рынок жизнь новый американский\n",
    "topic_5: проект nn технология запуск москва россия развитие\n",
    "topic_6: россия российский украина военный новый nn правительство\n",
    "topic_7: тело пациент штат мужчина житель задержать жертва\n",
    "topic_8: закон медицина песок ндс пресссекретарить поправка путин\n",
    "topic_9: поверхность век форум иск экипаж nn бизнесмен\n",
    "topic_10: район пострадать температура миссия произойти восток высота\n",
    "topic_11: россия газ гражданин российский украина глава nn\n",
    "topic_12: ск работодатель мэй калинин тверской телевизионный элементарный\n",
    "topic_13: банк экономика квартира налог русский рейс зарплата\n",
    "topic_14: млрд конкурс испытание супруг мальчик ребёнок внедрение\n",
    "topic_15: журнал nn рак фестиваль место мероприятие участник\n",
    "topic_16: система строительство станция млн пенсия двигатель проект\n",
    "topic_17: ракета завод экономика производить оплата индия млрд\n",
    "topic_18: северный инвестиция офицер китай значительно корея южный\n",
    "topic_19: улица дождь тепло сближение су небо таиланд\n",
    "topic_20: млрд система экономический цена рост новый земля\n",
    "topic_21: ребёнок всё выяснить очень болезнь nn врач\n",
    "topic_22: памятник египет век конструкция область дыра зуб\n",
    "topic_23: погибнуть земля мозг nn первый час минута\n",
    "topic_24: остров японский вирус япония китайский китай грунт\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень неплохо - большинство тем вполне можно описать о чем они"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем функцию, которая будет нам возвращать векторное представление новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18000</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.150157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.062583</td>\n",
       "      <td>0.432903</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.314745</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025063</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24000</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214919</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.053564</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.027522</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066356</td>\n",
       "      <td>0.250632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9000</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.277325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.493794</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055302</td>\n",
       "      <td>0.161575</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.295843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.163972</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.054021</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.135916</td>\n",
       "      <td>0.122011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.058533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427620</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.413499</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_id  topic_0   topic_1  topic_2  topic_3  topic_4   topic_5  \\\n",
       "18000       0      0.0  0.000000      0.0      0.0      0.0  0.000000   \n",
       "24000       1      0.0  0.214919      0.0      0.0      0.0  0.053564   \n",
       "9000        2      0.0  0.000000      0.0      0.0      0.0  0.277325   \n",
       "21000       3      0.0  0.295843      0.0      0.0      0.0  0.000000   \n",
       "3000        4      0.0  0.000000      0.0      0.0      0.0  0.000000   \n",
       "\n",
       "       topic_6   topic_7   topic_8  ...  topic_15  topic_16  topic_17  \\\n",
       "18000      0.0  0.000000  0.150157  ...  0.000000       0.0  0.000000   \n",
       "24000      0.0  0.027522  0.000000  ...  0.000000       0.0  0.000000   \n",
       "9000       0.0  0.000000  0.000000  ...  0.000000       0.0  0.493794   \n",
       "21000      0.0  0.163972  0.000000  ...  0.017414       0.0  0.054021   \n",
       "3000       0.0  0.427620  0.000000  ...  0.000000       0.0  0.000000   \n",
       "\n",
       "       topic_18  topic_19  topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "18000  0.062583  0.432903  0.000000  0.314745       0.0  0.025063  0.000000  \n",
       "24000  0.000000  0.000000  0.066356  0.250632       0.0  0.000000  0.030074  \n",
       "9000   0.000000  0.000000  0.055302  0.161575       0.0  0.000000  0.000000  \n",
       "21000  0.000000  0.000000  0.135916  0.122011       0.0  0.000000  0.058533  \n",
       "3000   0.000000  0.413499  0.000000  0.000000       0.0  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.sort_values(by=['doc_id']).head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_text = clean_text('Полузащитник сборной Бельгии Торган Азар не сыграет против Финляндии в заключительном матче группового этапа на Евро-2020. Об этом сообщает пресс-служба бельгийской команды')\n",
    "my_text = clean_text('Астрономы Европейской Южной обсерватории объявили об обнаружении в Солнечной системе новой карликовой планеты Гигея - самого крошечного из известных нам подобных объектов')\n",
    "#my_text = clean_text('Последний день налоговых выплат (платежи по налогу на прибыль) вчера поддерживал спрос на российскую валюту, не давая ей серьезно просесть на фоне укрепляющегося доллара')\n",
    "\n",
    "my_text = lemmatization(my_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.32842171 0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.28047299\n",
      " 0.         0.         0.         0.         0.         0.30309573\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.        ]\n",
      "0.3284217119216919\n",
      "1\n",
      "(1, ['россия', 'nn', 'исследование', 'всё', 'новый', 'проект', 'случай'])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "v = get_lda_vector(my_text)\n",
    "print(v)\n",
    "print(numpy.amax(v))\n",
    "print(numpy.where(v==numpy.amax(v))[0][0])\n",
    "print(topics_words[numpy.where(v==numpy.amax(v))[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущее значение:\n",
    "\\[0.         0.         0.         0.15950345 0.         0.10139688\n",
    " 0.36453602 0.         0.         0.         0.         0.\n",
    " 0.         0.08615274 0.         0.         0.08702044 0.\n",
    " 0.         0.         0.         0.15070353 0.         0.\n",
    " 0.        \\]\n",
    "0.36453601717948914\n",
    "6\n",
    "(6, \\['россия', 'российский', 'украина', 'военный', 'новый', 'nn', 'правительство'\\])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(common_corpus, smartirs='ntc')\n",
    "ldatfidf = LdaModel(tfidf[common_corpus], num_topics=25, id2word=common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, 0.86203086)]\n"
     ]
    }
   ],
   "source": [
    "print(ldatfidf[tfidf[other_corpus[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: наука мозг земля ракета военный автор обнаружить\n",
      "topic_1: туризм визовый автобус доля студия ледовый таможенный\n",
      "topic_2: доллар египетский продукция марафон бегун астахов забег\n",
      "topic_3: водитель ливия мэрия волгоград доска мемориальный конго\n",
      "topic_4: болезнь двигатель турецкий падение активность наиболее километр\n",
      "topic_5: научный статья млрд объём рынок организм солнце\n",
      "topic_6: чен оскар березовский бут ереван ай франсуа\n",
      "topic_7: население отель товар фильм питание сигнал собственность\n",
      "topic_8: россия проект российский украина nn новый млн\n",
      "topic_9: экономика пенсия снижение регион налог выплата механизм\n",
      "topic_10: экипаж забастовка ресторан флот король пресссекретарить офицер\n",
      "topic_11: рейс доход центр путин вырасти повышение праздник\n",
      "topic_12: технология запуск мобильный дания информационный макаров попов\n",
      "topic_13: влиять предприниматель изучить реакция формирование лодка парламент\n",
      "topic_14: сша система тыс американский цена рост вывод\n",
      "topic_15: шотландия сенатор си картер джексон хэнкок блэк\n",
      "topic_16: фестиваль газ квартира постановка иркутск алтай калининград\n",
      "topic_17: житель век террорист боевик культурный чечня видео\n",
      "topic_18: пляж смерть погибнуть актёр саммит метод работник\n",
      "topic_19: гостиница родитель лагерь кремль ндс перевезти фотография\n",
      "topic_20: станция гражданин курение энергия курить метро железнодорожный\n",
      "topic_21: агентство район препарат пострадать примерно обнаружить авария\n",
      "topic_22: музыка вино брюссель регион ремонт ресурс фрагмент\n",
      "topic_23: исследование рак взрыв вуз сон городской восток\n",
      "topic_24: фонд журнал опубликовать конкурс тело кровь налоговый\n"
     ]
    }
   ],
   "source": [
    "xtfidf = ldatfidf.show_topics(num_topics=25, num_words=7, formatted=False)\n",
    "topics_words_tfidf = [(tp[0], [wd[0] for wd in tp[1]]) for tp in xtfidf]\n",
    "\n",
    "for topic,words in topics_words_tfidf:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lda_tfidf_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = ldatfidf[tfidf[unseen_doc]]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.862031</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.684594</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.010236</td>\n",
       "      <td>0.079981</td>\n",
       "      <td>0.010236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.133359</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.492951</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.596572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375979</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  \\\n",
       "0       6  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1    4896  0.010236  0.010236  0.010236  0.010236  0.010236  0.010236   \n",
       "2    4897  0.000000  0.000000  0.000000  0.000000  0.000000  0.133359   \n",
       "3    4898  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4    4899  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "    topic_6   topic_7   topic_8  ...  topic_15  topic_16  topic_17  topic_18  \\\n",
       "0  0.000000  0.000000  0.862031  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.010236  0.010236  0.684594  ...  0.010236  0.010236  0.010236  0.010236   \n",
       "2  0.000000  0.000000  0.492951  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000  0.596572  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "4  0.000000  0.000000  0.375979  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "   topic_19  topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.010236  0.010236  0.010236  0.010236  0.079981  0.010236  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix_tfidf = pd.DataFrame([get_lda_tfidf_vector(text) for text in news['title'].values])\n",
    "topic_matrix_tfidf.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix_tfidf['doc_id'] = news['doc_id'].values\n",
    "topic_matrix_tfidf = topic_matrix_tfidf[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix_tfidf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['астроном',\n",
       " 'европейский',\n",
       " 'южный',\n",
       " 'обсерватория',\n",
       " 'объявить',\n",
       " 'обнаружение',\n",
       " 'солнечный',\n",
       " 'система',\n",
       " 'новый',\n",
       " 'карликовый',\n",
       " 'планета',\n",
       " 'гигей',\n",
       " 'крошечный',\n",
       " 'известный',\n",
       " 'подобный',\n",
       " 'объект']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#my_text = clean_text('Полузащитник сборной Бельгии Торган Азар не сыграет против Финляндии в заключительном матче группового этапа на Евро-2020. Об этом сообщает пресс-служба бельгийской команды')\n",
    "my_text = clean_text('Астрономы Европейской Южной обсерватории объявили об обнаружении в Солнечной системе новой карликовой планеты Гигея - самого крошечного из известных нам подобных объектов')\n",
    "#my_text = clean_text('Последний день налоговых выплат (платежи по налогу на прибыль) вчера поддерживал спрос на российскую валюту, не давая ей серьезно просесть на фоне укрепляющегося доллара')\n",
    "\n",
    "my_text = lemmatization(my_text)\n",
    "my_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(8, ['россия', 'проект', 'российский', 'украина', 'nn', 'новый', 'млн'])\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "v = get_lda_tfidf_vector(my_text)\n",
    "print(numpy.where(v==numpy.amax(v))[0][0])\n",
    "print(topics_words_tfidf[numpy.where(v==numpy.amax(v))[0][0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
